{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from xml.dom import minidom\n",
    "from hazm import *\n",
    "\n",
    "\n",
    "negareshiMoreOne = ['==']\n",
    "def read_xml(path):\n",
    "    mydoc = minidom.parse(path)\n",
    "\n",
    "    items = mydoc.getElementsByTagName('text')\n",
    "\n",
    "    documents = []\n",
    "    for i in range(len(items)):\n",
    "        documents.append(items[i].firstChild.data)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(document):\n",
    "    word_count = {}\n",
    "    processed_doc = []\n",
    "    normalizer = Normalizer()\n",
    "    stemmer = Stemmer()\n",
    "    word_id = 0\n",
    "    \n",
    "    for text in document:\n",
    "        text_temp = normalizer.normalize(text)\n",
    "        # print(\"NORMALIZED \", text_temp)\n",
    "        text_temp = word_tokenize(text_temp)\n",
    "        # print(\"TOKENIZED \", text_temp)\n",
    "\n",
    "        text_noNegarshi = []\n",
    "        for t in text_temp:\n",
    "            if len(t) != 1 and not t in negareshiMoreOne:\n",
    "                text_noNegarshi.append(t)\n",
    "        # print(\"NO NEGARESHI \", text_noNegarshi)\n",
    "        \n",
    "        text_stemmed = []\n",
    "        for t in text_noNegarshi:\n",
    "            temp = stemmer.stem(t)\n",
    "            if temp not in word_count:\n",
    "                word_count[temp] = 1\n",
    "            else:\n",
    "                word_count[temp] = word_count[temp] + 1\n",
    "            text_stemmed.append(temp)\n",
    "            \n",
    "\n",
    "        # print(\"STEMMED \", text_stemmed)\n",
    "        processed_doc.append(text_stemmed)\n",
    "        \n",
    "    temp = sorted(word_count.items(), key=lambda s: -s[1])\n",
    "    cutThreshold = 40\n",
    "    badwords = [a[0] for a in temp[0:cutThreshold]]\n",
    "    temp_doc = []\n",
    "    \n",
    "    for i, page in enumerate(processed_doc):\n",
    "\n",
    "        temp_page = []\n",
    "        for word in page:\n",
    "            if not word in badwords:\n",
    "                temp_page.append(word)\n",
    "        temp_doc.append(temp_page)\n",
    "    \n",
    "  \n",
    "    return temp_doc, badwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "persian = read_xml('Persian.xml')\n",
    "num_postings = len(persian)\n",
    "persian, badwords = preprocess(persian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['در', 'به', 'از', 'که', 'اس', 'این', 'را', 'با', 'آن', 'رده', 'سال', 'شهر', 'یک', '</ref>', '//www', 'http', 'کشور', 'برا', 'نا', 'بر', 'ایر', 'تاریخ', '||', 'تا', 'شد', '===', 'میلاد', 'خود', '<ref>', 'دو', 'of', 'بود', 'کرد', 'نیز', 'دارد', 'یا', 'name=', '|-', '<ref', 'دیگر']\n",
      "[\"'''بازی\", \"فکری'''\", 'جمله', 'بازی', 'پیدا', 'اساس', 'فکر', 'تمرکز', 'انس', 'انجا', 'می\\u200cگیرد', 'جمله', 'بازی', 'فکر', 'می\\u200cتو', 'موارد', 'زیر', 'اشاره', 'پنتاگو', 'شطرنج', 'نرد', 'فکر', 'بکر', 'دیدار', 'قاتل', 'هشت\\u200cپا', 'ورق\\u200cباز', 'بیس', 'سوال', 'نقطه', 'باز', 'مافیا', 'باز', 'گروه', 'سودوکو', 'جدول', 'کل', 'متقاطع', 'مکعب', 'روبیک', 'چامپ', 'بازی', 'تخته', 'باز', 'ویدیو', 'رایانه', 'آتار', 'پانویس', 'پانویس', 'باز', 'خانگ', 'بازی-خرد', 'سرگرمی-خرد', 'بازی', 'فکر', 'مهارت']\n"
     ]
    }
   ],
   "source": [
    "print(badwords)\n",
    "print(persian[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# bigram = {}\n",
    "gram = {}\n",
    "word_id = 0\n",
    "# bi_id = 0\n",
    "\n",
    "for page_indx, page in enumerate(persian):\n",
    "    for word_indx, word in enumerate(page):\n",
    "        if word not in gram:\n",
    "            gram[word] = ([[] for _ in range(num_postings)], word_id)\n",
    "            word_id += 1           \n",
    "    \n",
    "        gram[word][0][page_indx].append(word_indx)\n",
    "\n",
    "#         if word_indx == len(page) - 1:\n",
    "#             continue\n",
    "#         next_word = page[word_indx + 1]\n",
    "#         biword = (word, next_word)\n",
    "\n",
    "#         if not biword in bigram:\n",
    "#             bigram[biword] = ([], bi_id)\n",
    "#             bi_id += 1\n",
    "#         bigram[biword][0].append(page_indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'با'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-648c86e4d710>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'با'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'با'"
     ]
    }
   ],
   "source": [
    "print(gram['با'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram = {}\n",
    "gram = {}\n",
    "word_id = 0\n",
    "# bi_id = 0\n",
    "\n",
    "for page_indx, page in enumerate(persian):\n",
    "    for word_indx, word in enumerate(page):\n",
    "        if word not in gram:\n",
    "            gram[word] = ([[] for _ in range(num_postings)], word_id)\n",
    "            word_id += 1           \n",
    "    \n",
    "        gram[word][0][page_indx].append(word_indx - gram[word][0][page_indx][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
